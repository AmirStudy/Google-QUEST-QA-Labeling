# Google QUEST Q&A Labeling - Planning
 
## Working cycle:
1. Read competition information and relevant content to feel comfortable with the problem. Create a hypothesis based on the problem.
2. Initial data exploration to feel comfortable with the problem and the data.
3. Build the first implementation (baseline).
4. Loop through [Analyze -> Approach(model) -> Implement -> Evaluate].

## 1. Literature review (read some kernels and relevant content related to the competition).
- ### Relevant content:
  - [Encoder-decoders in Transformers: a hybrid pre-trained architecture for seq2seq](https://medium.com/huggingface/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8)
  - [Universal Sentence Encoder - TF Hub](https://tfhub.dev/google/universal-sentence-encoder/4)
  - [Semantic Similarity with TF-Hub Universal Encoder - Colab](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)
  - [Multilingual Universal Sentence Encoder for Semantic Retrieval](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html)
  - [Universal Sentence Encoder - Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf)
  - [BERT - Input](https://huggingface.co/transformers/model_doc/bert.html#bertmodel)
  - [HuggingFace Transformers](https://huggingface.co/transformers/)
  - [Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5)
  - [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
  - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)
  - [Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0](https://medium.com/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a)
  - [Simple BERT using TensorFlow 2.0](https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)
  - [TF Hub - bert_en_uncased_L-12_H-768_A-12](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1)
  - [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
  - [Multilingual Universal Sentence Encoder for Semantic Retrieval](https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html)
  - [Semantic Similarity with TF-Hub Universal Encoder](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb)
  
- ### GitHub:
  - [Git - Transformers: State-of-the-art Natural Language Processing](https://github.com/huggingface/transformers)
  - [GloVe model for distributed word representation](https://github.com/stanfordnlp/GloVe)
  
- ### Papers:
  - [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/abs/1905.05583)
  - [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)
  - [Universal Sentence Encoder](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46808.pdf)
  
- ### Videos:
  - [BERT for Kaggle Competitions | Yuanhae Wu | Kaggle Days](https://www.youtube.com/watch?v=jS79Y8I0DF4&t=9s)
  - [Deep Learning Formulas for NLP Applications | Chenglong Chen | Kaggle](https://www.youtube.com/watch?v=SmsAI0kLJFc&t=0s)
  - [Solving NLP Problems with BERT | Yuanhao Wu | Kaggle](https://www.youtube.com/watch?v=rQQAIJIf60s)

- ### Kernels:

- ### Discussions:
  - [Everything you always wanted to know about BERT (but were afraid to ask)](https://www.kaggle.com/c/google-quest-challenge/discussion/128420)
  - [BERT & Friends Reference](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/126702)
  - [1ST PLACE SOLUTION - Jigsaw competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/103280#latest-619135)
  - [Let's Complicate Things [Bert]](https://www.kaggle.com/c/google-quest-challenge/discussion/123770)
 
- ### Insights:
 - #### Positive Insights
 - #### Negative Insights
